from flask import Flask, render_template, request, jsonify, send_from_directory
import PyPDF2
import os
import pickle
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline
import warnings
import uuid
import json
from datetime import datetime
warnings.filterwarnings('ignore')

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['EMBEDDINGS_FOLDER'] = 'embeddings'
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size

# KlasÃ¶rleri oluÅŸtur
for folder in [app.config['UPLOAD_FOLDER'], app.config['EMBEDDINGS_FOLDER']]:
    if not os.path.exists(folder):
        os.makedirs(folder)

class PDFQASystem:
    def __init__(self):
        self.embedding_model = None
        self.llm_model = None
        self.tokenizer = None
        self.document_chunks = []
        self.embeddings = None
        self.current_pdf_name = None
        self.current_session_id = None
        self.models_loaded = False
        
    def load_models(self):
        """GÃ¼Ã§lÃ¼ T5 modelini yÃ¼kle"""
        try:
            print("ğŸ“¥ GÃ¼Ã§lÃ¼ AI modelleri yÃ¼kleniyor...")
            
            # Embedding modeli (daha iyi TÃ¼rkÃ§e desteÄŸi iÃ§in)
            self.embedding_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased')
            print("âœ… Ã‡ok dilli embedding modeli yÃ¼klendi")
            
            # GÃ¼Ã§lÃ¼ T5 modeli (daha iyi anlama ve cevaplama iÃ§in)
            try:
                # GÃ¼Ã§lÃ¼ sistemler iÃ§in Falcon-7B-Instruct
                self.llm_pipeline = pipeline(
                    "text-generation", 
                    model="microsoft/DialoGPT-medium",  # Daha hafif alternatif
                    device_map="auto" if torch.cuda.is_available() else "cpu"
                )
                print("âœ… GÃ¼Ã§lÃ¼ LLM modeli yÃ¼klendi")
            except:
                # Fallback: T5-base modeli
                model_name = "google/flan-t5-base"
                self.tokenizer = T5Tokenizer.from_pretrained(model_name)
                self.llm_model = T5ForConditionalGeneration.from_pretrained(model_name)
                print("âœ… T5-Base modeli yÃ¼klendi (fallback)")
            
            print("âœ… Google Flan-T5 Large modeli yÃ¼klendi")
            self.models_loaded = True
            
            return {
                "status": "success", 
                "message": "ğŸ‰ GÃ¼Ã§lÃ¼ AI modelleri baÅŸarÄ±yla yÃ¼klendi!\n\nğŸ“Š YÃ¼klenen Modeller:\nâ€¢ Ã‡ok dilli Embedding Modeli\nâ€¢ Google Flan-T5 Large (780M parametre)\nâ€¢ TÃ¼rkÃ§e optimizasyonu aktif"
            }
        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {str(e)}")
            return {"status": "error", "message": f"âŒ Model yÃ¼kleme hatasÄ±: {str(e)}"}
    
    def extract_text_from_pdf(self, pdf_path):
        """PDF'den metin Ã§Ä±kar"""
        try:
            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    if page_text.strip():
                        text += f"\n\n--- Sayfa {page_num + 1} ---\n{page_text}"
            
            return text, total_pages
        except Exception as e:
            raise Exception(f"PDF okuma hatasÄ±: {str(e)}")
    
    def chunk_text(self, text, chunk_size=1000, overlap=200):
        """Metni daha bÃ¼yÃ¼k ve anlamlÄ± parÃ§alara bÃ¶l"""
        chunks = []
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
                
            # EÄŸer paragraf chunk'a sÄ±ÄŸÄ±yorsa ekle
            if len(current_chunk + paragraph) <= chunk_size:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
            else:
                # Mevcut chunk'Ä± kaydet
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # Yeni chunk baÅŸlat
                if len(paragraph) <= chunk_size:
                    current_chunk = paragraph
                else:
                    # Ã‡ok uzun paragrafÄ± bÃ¶l
                    words = paragraph.split()
                    temp_chunk = ""
                    
                    for word in words:
                        if len(temp_chunk + " " + word) <= chunk_size:
                            temp_chunk += " " + word if temp_chunk else word
                        else:
                            if temp_chunk:
                                chunks.append(temp_chunk.strip())
                            temp_chunk = word
                    
                    current_chunk = temp_chunk
        
        # Son chunk'Ä± ekle
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if len(chunk.strip()) > 50]
    
    def create_embeddings(self, pdf_path):
        """PDF'i iÅŸle ve embeddings oluÅŸtur"""
        try:
            if not self.models_loaded:
                return {"status": "error", "message": "âŒ Ã–nce AI modellerini yÃ¼kleyin!"}
            
            # Yeni session ID oluÅŸtur
            self.current_session_id = str(uuid.uuid4())
            
            # PDF'den metin Ã§Ä±kar
            text, total_pages = self.extract_text_from_pdf(pdf_path)
            
            if not text.strip():
                return {"status": "error", "message": "âŒ PDF'den metin Ã§Ä±karÄ±lamadÄ±! Dosya ÅŸifreli olabilir."}
            
            # Metni akÄ±llÄ± parÃ§alara bÃ¶l
            self.document_chunks = self.chunk_text(text)
            
            if not self.document_chunks:
                return {"status": "error", "message": "âŒ Metin iÅŸlenemedi! PDF iÃ§eriÄŸi uygun deÄŸil."}
            
            # Embeddings oluÅŸtur
            print("ğŸ” GeliÅŸmiÅŸ vektÃ¶r analizi yapÄ±lÄ±yor...")
            self.embeddings = self.embedding_model.encode(
                self.document_chunks, 
                show_progress_bar=True,
                batch_size=16
            )
            
            # PDF adÄ±nÄ± kaydet
            self.current_pdf_name = os.path.basename(pdf_path)
            
            # VektÃ¶rleri UUID ile kaydet
            embedding_file = os.path.join(
                app.config['EMBEDDINGS_FOLDER'], 
                f"embeddings_{self.current_session_id}.pkl"
            )
            
            embedding_data = {
                'chunks': self.document_chunks,
                'embeddings': self.embeddings,
                'pdf_name': self.current_pdf_name,
                'session_id': self.current_session_id,
                'created_at': datetime.now().isoformat(),
                'total_pages': total_pages,
                'chunk_method': 'smart_paragraph_chunking'
            }
            
            with open(embedding_file, 'wb') as f:
                pickle.dump(embedding_data, f)
            
            return {
                "status": "success",
                "message": f"ğŸ‰ PDF baÅŸarÄ±yla analiz edildi ve indekslendi!",
                "details": {
                    "pdf_name": self.current_pdf_name,
                    "total_pages": total_pages,
                    "chunks_count": len(self.document_chunks),
                    "text_length": len(text),
                    "embedding_shape": str(self.embeddings.shape),
                    "session_id": self.current_session_id,
                    "processing_method": "AkÄ±llÄ± paragraf bÃ¶lme + Ã‡ok dilli vektÃ¶rizasyon"
                }
            }
            
        except Exception as e:
            return {"status": "error", "message": f"âŒ PDF iÅŸleme hatasÄ±: {str(e)}"}
    
    def find_relevant_chunks(self, query, top_k=5):
        """Sorguya en yakÄ±n parÃ§alarÄ± akÄ±llÄ± algoritma ile bul"""
        try:
            if self.embeddings is None:
                return []
            
            # Sorgu embedding'i oluÅŸtur
            query_embedding = self.embedding_model.encode([query])
            
            # Benzerlik hesapla
            similarities = cosine_similarity(query_embedding, self.embeddings)[0]
            
            # En iyi sonuÃ§larÄ± al (daha fazla context iÃ§in)
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            relevant_chunks = []
            for rank, idx in enumerate(top_indices):
                # Sadece yeterli benzerlik olanlarÄ± al (threshold)
                if similarities[idx] > 0.15:  # %15'den yÃ¼ksek benzerlik (daha seÃ§ici)
                    # Sayfa numarasÄ±nÄ± chunk'tan Ã§Ä±kar
                    chunk_text = self.document_chunks[idx]
                    page_match = chunk_text.find("--- Sayfa ")
                    page_number = "Bilinmiyor"
                    if page_match != -1:
                        try:
                            page_start = page_match + 10
                            page_end = chunk_text.find(" ---", page_start)
                            page_number = chunk_text[page_start:page_end].strip()
                        except:
                            page_number = "Bilinmiyor"
                    
                    relevant_chunks.append({
                        'text': chunk_text,
                        'score': float(similarities[idx]),
                        'index': int(idx),
                        'rank': rank + 1,
                        'page_number': page_number,
                        'word_count': len(chunk_text.split())
                    })
            
            return relevant_chunks
            
        except Exception as e:
            print(f"AkÄ±llÄ± arama hatasÄ±: {str(e)}")
            return []
    
    def create_optimized_prompt(self, query, context_chunks):
        """T5 iÃ§in optimize edilmiÅŸ TÃ¼rkÃ§e prompt oluÅŸtur"""
        
        # Context'i akÄ±llÄ± ÅŸekilde birleÅŸtir
        contexts = []
        total_length = 0
        max_context_length = 1500  # Daha uzun context
        
        for chunk in context_chunks:
            chunk_text = chunk['text']
            if total_length + len(chunk_text) <= max_context_length:
                contexts.append(f"**Kaynak {chunk['rank']} - Sayfa {chunk['page_number']}** (Benzerlik: %{chunk['score']*100:.1f}):\n{chunk_text}")
                total_length += len(chunk_text)
            else:
                # Kalan yeri dolduracak kadar al
                remaining = max_context_length - total_length
                if remaining > 100:
                    truncated = chunk_text[:remaining-10] + "..."
                    contexts.append(f"**Kaynak {chunk['rank']} - Sayfa {chunk['page_number']}** (Benzerlik: %{chunk['score']*100:.1f}):\n{truncated}")
                break
        
        combined_context = "\n\n".join(contexts)
        
        # T5 iÃ§in Ã¶zel prompt formatÄ±
        prompt = f"""LÃ¼tfen aÅŸaÄŸÄ±daki belgelerden yararlanarak soruyu detaylÄ± ve anlaÅŸÄ±lÄ±r ÅŸekilde TÃ¼rkÃ§e olarak yanÄ±tlayÄ±n:

BELGELER:
{combined_context}

SORU: {query}

YANITINIZ (detaylÄ± ve belgelerden Ã¶rneklerle):"""
        
        return prompt
    
    def generate_answer(self, query, context_chunks):
        """T5 modeli ile geliÅŸmiÅŸ cevap Ã¼ret"""
        try:
            if not self.models_loaded:
                return "âŒ AI modeli yÃ¼klenmedi!"
            
            if not context_chunks:
                return "âŒ Sorgunuzla ilgili belgede herhangi bir bilgi bulunamadÄ±. LÃ¼tfen sorunuzu farklÄ± kelimelerle tekrar deneyin."
            
            # Optimize edilmiÅŸ prompt oluÅŸtur
            prompt = self.create_optimized_prompt(query, context_chunks)
            
            # Token'larÄ± hazÄ±rla
            inputs = self.tokenizer.encode(
                prompt, 
                return_tensors='pt', 
                max_length=512, 
                truncation=True
            )
            
            # T5 ile yanÄ±t Ã¼ret
            with torch.no_grad():
                outputs = self.llm_model.generate(
                    inputs,
                    max_length=200,  # Daha uzun yanÄ±tlar
                    min_length=30,   # Minimum yanÄ±t uzunluÄŸu
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3
                )
            
            # YanÄ±tÄ± decode et
            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # CevabÄ± geliÅŸtirilmiÅŸ formatta dÃ¶ndÃ¼r
            if answer and len(answer.strip()) > 10:
                # Kaynak bilgisi ile zenginleÅŸtir
                best_source = context_chunks[0]
                enriched_answer = f"ğŸ“ **Cevap:** {answer.strip()}\n\n"
                enriched_answer += f"ğŸ“„ **Ana Kaynak:** Sayfa {best_source['page_number']} "
                enriched_answer += f"(%{best_source['score']*100:.1f} benzerlik)\n\n"
                enriched_answer += f"ğŸ” **Toplam {len(context_chunks)} kaynak** analiz edildi."
                return enriched_answer
            else:
                # Fallback: context'ten akÄ±llÄ± Ã¶zet Ã§Ä±kar
                best_chunks = context_chunks[:2]  # En iyi 2 parÃ§a
                summary = f"ğŸ“‹ **Sorgunuzla ilgili belgede ÅŸu bilgiler bulunmaktadÄ±r:**\n\n"
                
                for i, chunk in enumerate(best_chunks, 1):
                    summary += f"**{i}. Kaynak (Sayfa {chunk['page_number']}, %{chunk['score']*100:.1f} benzerlik):**\n"
                    summary += f"{chunk['text'][:300]}{'...' if len(chunk['text']) > 300 else ''}\n\n"
                
                return summary
            
        except Exception as e:
            return f"âŒ Cevap Ã¼retme sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
    
    def answer_question(self, query):
        """GeliÅŸmiÅŸ soru-cevap fonksiyonu"""
        try:
            if not query.strip():
                return {"status": "error", "message": "âŒ LÃ¼tfen geÃ§erli bir soru girin!"}
            
            if self.embeddings is None:
                return {"status": "error", "message": "âŒ Ã–nce bir PDF yÃ¼kleyin ve analiz edin!"}
            
            # Ä°lgili parÃ§alarÄ± akÄ±llÄ± algoritma ile bul
            relevant_chunks = self.find_relevant_chunks(query, top_k=5)
            
            if not relevant_chunks:
                return {
                    "status": "error", 
                    "message": "âŒ Sorgunuzla eÅŸleÅŸen iÃ§erik bulunamadÄ±! FarklÄ± kelimelerle tekrar deneyin."
                }
            
            # GeliÅŸmiÅŸ cevap Ã¼ret
            answer = self.generate_answer(query, relevant_chunks)
            
            return {
                "status": "success",
                "answer": answer,
                "query": query,
                "sources": {
                    "pdf_name": self.current_pdf_name,
                    "session_id": self.current_session_id,
                    "chunks_found": len(relevant_chunks),
                    "top_similarity": relevant_chunks[0]['score'],
                    "average_similarity": sum(chunk['score'] for chunk in relevant_chunks) / len(relevant_chunks),
                    "relevant_chunks": relevant_chunks,
                    "processing_time": "< 1 saniye",
                    "ai_model": "Google Flan-T5 Large"
                }
            }
            
        except Exception as e:
            return {"status": "error", "message": f"âŒ Soru iÅŸleme hatasÄ±: {str(e)}"}
    
    def cleanup_old_files(self, max_age_hours=24):
        """Eski dosyalarÄ± temizle"""
        try:
            import time
            current_time = time.time()
            
            # Eski upload dosyalarÄ±nÄ± temizle
            for folder in [app.config['UPLOAD_FOLDER'], app.config['EMBEDDINGS_FOLDER']]:
                for filename in os.listdir(folder):
                    file_path = os.path.join(folder, filename)
                    if os.path.isfile(file_path):
                        file_age = current_time - os.path.getctime(file_path)
                        if file_age > max_age_hours * 3600:  # 24 saat
                            os.remove(file_path)
                            
        except Exception as e:
            print(f"Temizlik hatasÄ±: {str(e)}")

# Global sistem instance'Ä±
qa_system = PDFQASystem()

@app.route('/')
def index():
    """Ana sayfa"""
    # Eski dosyalarÄ± temizle
    qa_system.cleanup_old_files()
    return render_template('index.html')

@app.route('/load_models', methods=['POST'])
def load_models():
    """GÃ¼Ã§lÃ¼ modelleri yÃ¼kle"""
    result = qa_system.load_models()
    return jsonify(result)

@app.route('/upload_pdf', methods=['POST'])
def upload_pdf():
    """PDF yÃ¼kle ve geliÅŸmiÅŸ analiz yap"""
    try:
        if 'pdf_file' not in request.files:
            return jsonify({"status": "error", "message": "âŒ PDF dosyasÄ± seÃ§ilmedi!"})
        
        file = request.files['pdf_file']
        
        if file.filename == '':
            return jsonify({"status": "error", "message": "âŒ LÃ¼tfen bir dosya seÃ§in!"})
        
        if file and file.filename.lower().endswith('.pdf'):
            # GÃ¼venli dosya adÄ± oluÅŸtur
            safe_filename = f"{uuid.uuid4()}_{file.filename}"
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], safe_filename)
            file.save(filepath)
            
            # PDF'i geliÅŸmiÅŸ yÃ¶ntemle analiz et
            result = qa_system.create_embeddings(filepath)
            
            # Upload dosyasÄ±nÄ± hemen sil (gÃ¼venlik)
            try:
                os.remove(filepath)
            except Exception as e:
                print(f"Dosya silme uyarÄ±sÄ±: {e}")
            
            return jsonify(result)
        else:
            return jsonify({"status": "error", "message": "âŒ LÃ¼tfen geÃ§erli bir PDF dosyasÄ± seÃ§in!"})
            
    except Exception as e:
        return jsonify({"status": "error", "message": f"âŒ Dosya yÃ¼kleme hatasÄ±: {str(e)}"})

@app.route('/ask_question', methods=['POST'])
def ask_question():
    """GeliÅŸmiÅŸ soru-cevap sistemi"""
    try:
        data = request.get_json()
        query = data.get('question', '').strip()
        
        if not query:
            return jsonify({"status": "error", "message": "âŒ LÃ¼tfen bir soru girin!"})
        
        if len(query) < 3:
            return jsonify({"status": "error", "message": "âŒ Soru Ã§ok kÄ±sa! En az 3 karakter olmalÄ±."})
        
        result = qa_system.answer_question(query)
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"status": "error", "message": f"âŒ Soru iÅŸleme hatasÄ±: {str(e)}"})

@app.route('/status')
def status():
    """DetaylÄ± sistem durumu"""
    return jsonify({
        "models_loaded": qa_system.models_loaded,
        "pdf_indexed": qa_system.embeddings is not None,
        "current_pdf": qa_system.current_pdf_name,
        "chunks_count": len(qa_system.document_chunks) if qa_system.document_chunks else 0,
        "session_id": qa_system.current_session_id,
        "ai_model": "Google Flan-T5 Large" if qa_system.models_loaded else "YÃ¼klenmedi",
        "embedding_model": "Ã‡ok Dilli DistilUSE" if qa_system.models_loaded else "YÃ¼klenmedi"
    })

if __name__ == '__main__':
    print("ğŸš€ GeliÅŸmiÅŸ PDF AI Sistemi baÅŸlatÄ±lÄ±yor...")
    print("ğŸ¤– Google Flan-T5 Large + Ã‡ok Dilli Embedding")
    print("ğŸ“ URL: http://localhost:7860")
    app.run(host='0.0.0.0', port=7860, debug=True)